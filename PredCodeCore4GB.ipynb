{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, avg, when\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import hdf5_getters\n",
    "import re\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://host-192-168-1-153-ldsa:7077\") \\\n",
    "        .appName(\"SongHotness4GB-Team7\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.executor.instances\",4)\\\n",
    "        .config('spark.executor.memory','8g')\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\",4)\\\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\",4)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "sqlContext = SQLContext(spark_context)\n",
    "\n",
    "def rowUnpack(df):    #Function to unpack average tuple\n",
    "        results={}\n",
    "        for i in df:\n",
    "            results.update(i.asDict())\n",
    "        return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Song:            #Class for extracting song from h5 files\n",
    "    songCount = 0\n",
    "    # songDictionary = {}\n",
    "\n",
    "    def __init__(self, songID):\n",
    "        self.id = songID\n",
    "        Song.songCount += 1\n",
    "        # Song.songDictionary[songID] = self\n",
    "\n",
    "        self.albumName = None\n",
    "        self.albumID = None\n",
    "        self.artistID = None\n",
    "        self.artistLatitude = None\n",
    "        self.artistLocation = None\n",
    "        self.artistLongitude = None\n",
    "        self.artistName = None\n",
    "        self.danceability = None\n",
    "        self.duration = None\n",
    "        self.energy = None\n",
    "        self.genreList = []\n",
    "        self.keySignature = None\n",
    "        self.keySignatureConfidence = None\n",
    "        self.lyrics = None\n",
    "        self.popularity = None\n",
    "        self.songhotttnesss = None\n",
    "        self.tempo = None\n",
    "        self.timeSignature = None\n",
    "        self.timeSignatureConfidence = None\n",
    "        self.title = None\n",
    "        self.year = None\n",
    "\n",
    "    def displaySongCount(self):\n",
    "        print(\"Total Song Count %i\" % Song.songCount)\n",
    "\n",
    "    def displaySong(self):\n",
    "        print(\"ID: %s\" % self.id)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This code has been adapted for conversion of .h5 files to PySpark Dataframe by Karthik Nair(3 June 2019) of Uppsala University (github.com/karnair)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "\"\"\"\n",
    "Original:\n",
    "\n",
    "Alexis Greenstreet (October 4, 2015) University of Wisconsin-Madison\n",
    "\n",
    "This code is designed to convert the HDF5 files of the Million Song Dataset\n",
    "to a CSV by extracting various song properties.\n",
    "\n",
    "The script writes to a \"SongCSV.csv\" in the directory containing this script.\n",
    "\n",
    "Please note that in the current form, this code only extracts the following\n",
    "information from the HDF5 files:\n",
    "AlbumID, AlbumName, ArtistID, ArtistLatitude, ArtistLocation,\n",
    "ArtistLongitude, ArtistName, Danceability, Duration, KeySignature,\n",
    "KeySignatureConfidence, SongID, Tempo, TimeSignature,\n",
    "TimeSignatureConfidence, Title, and Year.\n",
    "\n",
    "This file also requires the use of \"hdf5_getters.py\", written by\n",
    "Thierry Bertin-Mahieux (2010) at Columbia University\n",
    "\n",
    "Credit:\n",
    "This HDF5 to CSV code makes use of the following example code provided\n",
    "at the Million Song Dataset website \n",
    "(Home>Tutorial/Iterate Over All Songs, \n",
    "http://labrosa.ee.columbia.edu/millionsong/pages/iterate-over-all-songs),\n",
    "Which gives users the following code to get all song titles:\n",
    "\n",
    "#import os\n",
    "#import glob\n",
    "#import hdf5_getters\n",
    "#def get_all_titles(basedir,ext='.h5') :\n",
    "#    titles = []\n",
    "#   for root, dirs, files in os.walk(basedir):\n",
    "#        files = glob.glob(os.path.join(root,'*'+ext))\n",
    "#        for f in files:\n",
    "#            h5 = hdf5_getters.open_h5_file_read(f)\n",
    "#            titles.append( hdf5_getters.get_title(h5) )\n",
    "#            h5.close()\n",
    "#   return titles \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"This code has been modified for python3 compatibility by Karthik Nair(28 May 2019) of Uppsala University (github.com/karnair)\"\"\"\n",
    "\"\"\"This code has been adapted for conversion of .h5 files to PySpark Dataframe by Karthik Nair(3 June 2019) of Uppsala University (github.com/karnair)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5toPysDf(x):        #Reads and creates h5 files to pyspark dataframe. Replace 'x' with filesize in GB for use\n",
    "    \n",
    "    song_number = []\n",
    "    album_id = []\n",
    "    artist_latitude = []\n",
    "    artist_location = []\n",
    "    artist_longitude = []\n",
    "    artist_name =[]\n",
    "    danceability = []\n",
    "    duration = []\n",
    "    energy = []\n",
    "    key_signature = []\n",
    "    key_signature_confidence = []\n",
    "    song_id = []\n",
    "    tempo = []\n",
    "    song_hotttnesss = []\n",
    "    time_signature = []\n",
    "    time_signature_confidence = []\n",
    "    title = []\n",
    "    year = []\n",
    "    TotalFileSize = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    csvRowString = (\"SongID,AlbumID,ArtistLatitude,ArtistLocation,\"+\n",
    "            \"ArtistLongitude,ArtistName,Danceability,Duration,Energy,KeySignature,\"+\n",
    "            \"KeySignatureConfidence,SongHotttnesss,Tempo,TimeSignature,TimeSignatureConfidence,\"+\n",
    "            \"Title,Year\")\n",
    "    \n",
    "    csvAttributeList = re.split('\\W+', csvRowString)\n",
    "    \n",
    "    for i, v in enumerate(csvAttributeList):\n",
    "        csvAttributeList[i] = csvAttributeList[i].lower()\n",
    "        csvRowString = \"\"\n",
    "    \n",
    "    #basedir = \"/home/ubuntu/MillionSongSubset/data\" # \".\" As the default means the current directory\n",
    "    basedir = \"/mnt/ms/data\"\n",
    "    ext = \".h5\"\n",
    "    \n",
    "  \n",
    "    \n",
    "    for root, dirs, files in os.walk(basedir):        \n",
    "        files = glob.glob(os.path.join(root,'*'+ext))\n",
    "\n",
    "        for f in files:\n",
    "            if TotalFileSize >= x:            #Condition for limiting filesize\n",
    "                break\n",
    "            #print(f)\n",
    "            \n",
    "            #Reads and calculates file size in GB\n",
    "            filesize = os.stat(f)\n",
    "            filesize = float((filesize.st_size)/1073741824)\n",
    "            TotalFileSize = TotalFileSize + filesize\n",
    "            \n",
    "            \n",
    "            songH5File = hdf5_getters.open_h5_file_read(f)\n",
    "            song = Song(str(hdf5_getters.get_song_id(songH5File)))\n",
    "            testDanceability = hdf5_getters.get_danceability(songH5File)\n",
    "            # print type(testDanceability)\n",
    "            # print (\"Here is the danceability: \") + str(testDanceability)\n",
    "\n",
    "            #song.artistID = str(hdf5_getters.get_artist_id(songH5File))\n",
    "            song.albumID = str(hdf5_getters.get_release_7digitalid(songH5File))\n",
    "            #song.albumName = str(hdf5_getters.get_release(songH5File))\n",
    "            song.artistLatitude = str(hdf5_getters.get_artist_latitude(songH5File))\n",
    "            song.artistLocation = str(hdf5_getters.get_artist_location(songH5File))\n",
    "            song.artistLongitude = str(hdf5_getters.get_artist_longitude(songH5File))\n",
    "            song.artistName = str(hdf5_getters.get_artist_name(songH5File))\n",
    "            song.danceability = float(hdf5_getters.get_danceability(songH5File))\n",
    "            song.duration = float(hdf5_getters.get_duration(songH5File))\n",
    "            song.energy = float(hdf5_getters.get_energy(songH5File))\n",
    "            # song.setGenreList()\n",
    "            song.keySignature = float(hdf5_getters.get_key(songH5File))\n",
    "            song.keySignatureConfidence = float(hdf5_getters.get_key_confidence(songH5File))\n",
    "            # song.lyrics = None\n",
    "            # song.popularity = None\n",
    "            song.tempo = float(hdf5_getters.get_tempo(songH5File))\n",
    "            song.songhotttnesss = float(hdf5_getters.get_song_hotttnesss(songH5File))\n",
    "            song.timeSignature = float(hdf5_getters.get_time_signature(songH5File))\n",
    "            song.timeSignatureConfidence = float(hdf5_getters.get_time_signature_confidence(songH5File))\n",
    "            song.title = str(hdf5_getters.get_title(songH5File))\n",
    "            song.year = str(hdf5_getters.get_year(songH5File))\n",
    "        \n",
    "        \n",
    "        \n",
    "           #csvRowString += str(song.songCount) + \",\"\n",
    "            song_number.append(song.songCount)\n",
    "\n",
    "            for attribute in csvAttributeList:\n",
    "                if attribute == 'AlbumID'.lower():\n",
    "                    album_id.append(song.albumID)\n",
    "                elif attribute == 'ArtistLatitude'.lower():\n",
    "                    latitude = song.artistLatitude\n",
    "                    if latitude == 'nan':\n",
    "                        latitude = ''\n",
    "                    artist_latitude.append(latitude)\n",
    "                elif attribute == 'ArtistLocation'.lower():\n",
    "                    location = song.artistLocation\n",
    "                    location = location.replace(',','')\n",
    "                    artist_location.append(location) \n",
    "                elif attribute == 'ArtistLongitude'.lower():\n",
    "                    longitude = song.artistLongitude\n",
    "                    if longitude == 'nan':\n",
    "                        longitude = ''\n",
    "                    artist_longitude.append(longitude)   \n",
    "                elif attribute == 'ArtistName'.lower():\n",
    "                    artist_name.append(song.artistName)\n",
    "                elif attribute == 'Danceability'.lower():\n",
    "                    danceability.append(song.danceability)\n",
    "                elif attribute == 'Duration'.lower():\n",
    "                    duration.append(song.duration)\n",
    "                elif attribute == 'Energy'.lower():\n",
    "                    energy.append(song.energy)\n",
    "                elif attribute == 'KeySignature'.lower():\n",
    "                    key_signature.append(song.keySignature)\n",
    "                elif attribute == 'KeySignatureConfidence'.lower():\n",
    "                    # print \"key sig conf: \" + song.timeSignatureConfidence                                 \n",
    "                    key_signature_confidence.append(song.keySignatureConfidence)\n",
    "                elif attribute == 'SongID'.lower():\n",
    "                    song_id.append(song.id)\n",
    "                elif attribute == 'Tempo'.lower():\n",
    "                    # print \"Tempo: \" + song.tempo\n",
    "                    tempo.append(song.tempo)\n",
    "                elif attribute == 'SongHotttnesss'.lower():\n",
    "                    song_hotttnesss.append(song.songhotttnesss)\n",
    "                elif attribute == 'TimeSignature'.lower():\n",
    "                    time_signature.append(song.timeSignature)\n",
    "                elif attribute == 'TimeSignatureConfidence'.lower():\n",
    "                    # print \"time sig conf: \" + song.timeSignatureConfidence                                   \n",
    "                    time_signature_confidence.append(song.timeSignatureConfidence)\n",
    "                elif attribute == 'Title'.lower():\n",
    "                    title.append(song.title)\n",
    "                elif attribute == 'Year'.lower():\n",
    "                    year.append(song.year)\n",
    "                #\"\"\"else:\n",
    "                 #   csvRowString += \"Erm. This didn't work. Error. :( :(\\n\" \"\"\"\"\n",
    "\n",
    "            #csvRowString += \",\"\n",
    "            songH5File.close()\n",
    "\n",
    "            \n",
    "        \n",
    "    pysp_df = sqlContext.createDataFrame(zip(song_number, album_id, artist_latitude, artist_location, artist_longitude, artist_name, danceability, duration, energy, key_signature, key_signature_confidence, song_id, tempo, song_hotttnesss, time_signature, time_signature_confidence, title, year), schema=['song_number', 'album_id', 'artist_latitude', 'artist_location', 'artist_longitude', 'artist_name', 'danceability', 'duration', 'energy', 'key_signature', 'key_signature_confidence', 'song_id', 'tempo', 'song_hotttnesss', 'time_signature', 'time_signature_confidence', 'title', 'year'])\n",
    "    print(TotalFileSize)\n",
    "    return pysp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.000016679055989\n"
     ]
    }
   ],
   "source": [
    "#song_number, album_id, artist_latitude, artist_location, artist_longitude, artist_name, danceability, duration, energy, key_signature, key_signature_confidence, song_id, tempo, song_hotttnesss, time_signature, time_signature_confidence, title, year = main()\n",
    "\n",
    "msd_df = h5toPysDf(2) #Number denotes the subset size in GBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execution Time Start\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Replace NaN with 0\n",
    "msd_df = msd_df.na.fill(0)\n",
    "msd_df1 = msd_df.filter(msd_df['song_hotttnesss'] > 0).agg(avg(col(\"song_hotttnesss\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Average\n",
    "\n",
    "average = rowUnpack(msd_df1)\n",
    "average = average['avg(song_hotttnesss)']\n",
    "\n",
    "#Replace NaN and Zeroes with Average\n",
    "msd_df_corrected = msd_df.withColumn(\"song_hotttnesss\", \\\n",
    "              when(msd_df[\"song_hotttnesss\"] == 0, average).otherwise(msd_df[\"song_hotttnesss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msd_df_corrected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_df_sub = msd_df_corrected.select(\"duration\",\"key_signature\",\"tempo\",\"time_signature\",\"song_hotttnesss\")\n",
    "#msd_df_sub.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting\n",
    "\n",
    "msd_sort = msd_df_sub.orderBy('duration', ascending=True)\n",
    "#msd_sort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data into Training and Testing\n",
    "\n",
    "(training_data, test_data) = msd_sort.randomSplit([0.7, 0.3])\n",
    "training_data =training_data.rdd.map(lambda x: LabeledPoint(x[4], x[:4]))\n",
    "test_data =test_data.rdd.map(lambda x: LabeledPoint(x[4], x[:4]))\n",
    "\n",
    "#training_data.take(5)\n",
    "#test_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainDecisionTree\n",
    "model = DecisionTree.trainRegressor(training_data, categoricalFeaturesInfo={},\n",
    "                                    impurity='variance', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = model.toDebugString()\n",
    "\n",
    "#Test Decision Tree\n",
    "predictions = model.predict(test_data.map(lambda x: x.features))\n",
    "labelsAndPredictions = test_data.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "#Test Accuracy by Mean Squared Error\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() / float(test_data.count())\n",
    "\n",
    "end_time = time.time()\n",
    "exec_time = end_time - start_time    #Calculate Execution Time\n",
    "\n",
    "\n",
    "print('Test Mean Squared Error = ' + str(testMSE))\n",
    "print(\"--- %s seconds ---\" % exec_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Beautify Decision Tree\n",
    "input_cols = [\"duration\",\"key_signature\",\"tempo\",\"time_signature\"]\n",
    "\n",
    "for i, feat in enumerate(input_cols):\n",
    "    model_tree = model_tree.replace('feature ' + str(i), feat)\n",
    "\n",
    "print('Learned regression tree model: \\n')\n",
    "print(model_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(spark_context, \"/home/ubuntu/MillionSongLDSA\")\n",
    "#sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeRegressionModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
